{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eafa0aa",
   "metadata": {},
   "source": [
    "### **Linear Regression**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9b2f1a",
   "metadata": {},
   "source": [
    "\n",
    "### simple Linear Regression\n",
    "\n",
    "Given a dataset of n points: (x1,y1), (x2,y2), …, (xn,yn)\n",
    "\n",
    "we know that , y= mx+c or b1x+b0\n",
    "\n",
    "\n",
    "##### Loss Function: Mean Squared Error (MSE)\n",
    "\n",
    "The MSE loss function is:\n",
    "\n",
    "$$\n",
    "L(b_0, b_1) = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\frac{1}{n} \\sum_{i=1}^n (y_i - (b_0 + b_1 x_i))^2\n",
    "$$\n",
    "\n",
    "\n",
    "***Derivatives***\n",
    "To use gradient descent, we need the partial derivatives:\n",
    "\n",
    "**Derivative w\\.r.t $b_0$:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b_0} = \\frac{-2}{n} \\sum_{i=1}^n (y_i - (b_0 + b_1 x_i))\n",
    "$$\n",
    "\n",
    "**Derivative w\\.r.t $b_1$:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b_1} = \\frac{-2}{n} \\sum_{i=1}^n x_i (y_i - (b_0 + b_1 x_i))\n",
    "$$\n",
    "\n",
    "\n",
    "after sub to 0 we get:\n",
    "\n",
    "1. Slope ( $b_1$ ):\n",
    "\t- m = y2-y1/x2-x1 \n",
    "\t- $b_1$ = $\\Large \\frac{Cov(x,y)}{Var(x)}$ = $\\large \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2}$ = $\\Large {\\frac{\\sum x_{i}y_{i}-n{\\bar{x}}{\\bar{y}}}{\\sum x_{i}^{2}-n{\\bar{x}}^{2}}}$\n",
    " 2. Intercept ( $b_0$ ): \n",
    "\t - $b_0= \\bar{y} - b_1 \\bar{x}$\n",
    " \n",
    "\n",
    "### Multiple Linear Regression Model:\n",
    "\n",
    "This approach treats the data as a matrix and uses linear algebra operations to estimate the optimal values for the coefficients\n",
    "\n",
    " **Matrix Formulation** :Y=Xb+ε\n",
    "Where:\n",
    "- Y is an n×1 vector of outputs,\n",
    "- X is an n×(p+1) **design matrix** (first column is all 1’s for intercept),\n",
    "- b is a (p+1)×1 vector of coefficients (including b0),\n",
    "- ε is the error vector.\n",
    "\n",
    "$Y = b_0 + b_1X_1 + b_2X_2 + \\varepsilon$\n",
    "\n",
    "You'd build a design matrix like this:\n",
    "\n",
    "$X= = \\begin{bmatrix} 1 & x_{11} & x_{12} \\\\ 1 & x_{21} & x_{22} \\\\ \\vdots & \\vdots & \\vdots \\\\ 1 & x_{n1} & x_{n2} \\end{bmatrix}$\n",
    "\n",
    "\n",
    "$\\hat{\\mathbf{b}} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{Y}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0491101",
   "metadata": {},
   "source": [
    "### **Logistic Regresssion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db8b65f",
   "metadata": {},
   "source": [
    "\n",
    "$P(y = 1 \\mid x) = \\sigma(\\hat y) = \\frac{1}{1 + e^{- \\hat y}}$\n",
    "\n",
    "\n",
    "1. **Log-Likelihood**\n",
    "\n",
    "\t$\\ell(\\beta) = \\log L(\\beta) = \\sum_{i=1}^{n} \\left[ y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) \\right]$\n",
    "\n",
    "\tSubstitute: $p_i = \\sigma(x_i^\\top \\beta)$\n",
    "\t\n",
    "\t$\\ell(\\beta) = \\sum_{i=1}^{n} \\left[ y_i \\log(\\sigma(x_i^\\top \\beta)) + (1 - y_i) \\log(1 - \\sigma(x_i^\\top \\beta)) \\right]$\n",
    "\n",
    "2. **Gradient of Log-Likelihood**\n",
    "\t\n",
    "\tTo find parameters, we take the gradient of the log-likelihood and set it to zero:\n",
    "\t\n",
    "\t$\\nabla_\\beta \\ell(\\beta) = \\sum_{i=1}^{n} (y_i - \\sigma(x_i^\\top \\beta)) x_i$\n",
    "\t\n",
    "\tSo: $\\boxed{\\nabla_\\beta \\ell(\\beta) = X^\\top (y - p)}$\n",
    "\t\n",
    "\tWhere:\n",
    "\t\n",
    "\t- $X \\in \\mathbb{R}^{n \\times d}$ is the design matrix,\n",
    "\t- $y \\in \\mathbb{R}^n$ is the vector of labels,\n",
    "\t- $p \\in \\mathbb{R}^n$ is the vector of predicted probabilities.\n",
    "\t    \n",
    "\t\n",
    "\tThis gradient is used in optimization algorithms (like **gradient ascent** or more often **Newton-Raphson** or **BFGS**) to find the optimal β\\beta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ed3c8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Gradient Descent\n",
    "\n",
    "# Input: \n",
    "#     loss function J(β)\n",
    "#     Gradient ∇J(β)\n",
    "#     Learning rate α\n",
    "#     Initial parameters β = β₀\n",
    "#     Number of iterations N\n",
    "\n",
    "# For i = 1 to N do:\n",
    "#     Compute gradient: g = ∇J(β)\n",
    "#     Update parameters: β = β - α * g\n",
    "\n",
    "# Return β\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kaggle (Python 3.x)",
   "language": "python",
   "name": "kaggle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
